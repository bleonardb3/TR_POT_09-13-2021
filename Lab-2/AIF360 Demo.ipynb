{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting and Mitigating Bias Using AI Fairness 360\n",
    "## Using \"Adversarial Debiasing\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " Machine learning models are increasingly used to inform high stakes decisions about people. Although machine learning, by its very nature, is always a form of statistical discrimination, the discrimination becomes objectionable when it places certain privileged groups at systematic advantage and certain unprivileged groups at systematic disadvantage. Biases in training data, due to either prejudice in labels or under-/over-sampling, yields models with unwanted bias.\n",
    "\n",
    "The AI Fairness 360 Python package includes a comprehensive set of metrics for datasets and models to test for biases, explanations for these metrics, and algorithms to mitigate bias in datasets and models. The AI Fairness 360 interactive demo provides a gentle introduction to the concepts and capabilities. The tutorials and other notebooks offer a deeper, data scientist-oriented introduction. The complete API is also available. \n",
    "\n",
    "For more information see links below:\n",
    "\n",
    "- AIF360 Demo: https://aif360.mybluemix.net\n",
    "- AIF360 GitHub: https://github.com/IBM/AIF360\n",
    "- AIF360 API Docs: https://aif360.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Objective\n",
    "\n",
    "In this notebook you will utilize AIF360 to detect and mitigate bias on the Compas dataset which is used to assess the likelihood that a criminal defendant will re-offend. \n",
    "\n",
    "Upon completing this lab you will learn how to:\n",
    "\n",
    "- Load datasets from the toolkit package\n",
    "- Check the dataset for bias\n",
    "- Mitigate existing bias in using Adversarial Debiasing technique\n",
    "- Train on both original and corrected dataset and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "This tutorial uses a Jupyter Notebook, an open-source web applicaiton that allows you to create and share documents that contain instructions as well as live code.\n",
    "\n",
    "The Jupyter Notebooks we are using today is based on a Watson Studio environment, a set of open source packages that provide us with a standardized data analysis tools. At multiple points during the demo, we will import additional tools we need for specific steps:\n",
    "\n",
    "E.g. `import pandas as pd` -> to import the \"pandas\" tool for data manipulation.\n",
    "\n",
    "E.g. `!pip install wget` -> to install a tool \"wget\" to download data from external webpages.\n",
    "\n",
    "Watson Studio also contains functionality that allows a user to pre-define a set of environments down to the package version level as well as define the hardware configurations available to certain users, allowing teams to easily standardize toolsets and resources. If needed, we can also connect our notebooks to GPUs, Apache Spark, and external clusters for higher performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Please follow the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Required Libraries and Functions\n",
    "Here we load the AI Fairness 360 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install colorama\n",
    "!pip uninstall tensorflow==2.1.0 -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard==1.15.0\n",
    "!pip install tensorflow-estimator==1.15.1\n",
    "!pip install tensorflow==1.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "import pandas as pd\n",
    " \n",
    "try:\n",
    "    from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing    \n",
    "except ModuleNotFoundError:\n",
    "        print(\"Caught Module not Found - This is ok\")\n",
    "        \n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "from aif360.datasets import BinaryLabelDataset    \n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.metrics import ClassificationMetric  \n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# to plot graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from colorama import Fore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Importing COMPAS Dataset\n",
    "\n",
    "Across the nation, judges, probation and parole officers are increasingly using algorithms to assess a criminal defendant’s likelihood of becoming a recidivist – a term used to describe criminals who re-offend. One of the tools used is called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions) and the dataset it uses is the Compas dataset which you will be working with in this lab. By analyzing this datset, it was found that African-American defendants were far more likely than Caucasian defendants to be incorrectly judged to be at a higher risk of recidivism, while Caucasian defendants were more likely than African-American defendants to be incorrectly flagged as low risk, therefore, this dataset is biased.\n",
    "\n",
    "To perform bias detection and mitigation using AIF360, the toolkit needs to be tailored to the particular bias of interest. More specifically, it needs to know the attribute or attributes to track, called \"protected attributes\", that are of interest: in this dataset, \"race\" and \"sex\" are protected attributes.\n",
    "\n",
    "For purpose of this lab, we choose the \"Race\" attribute.\n",
    "\n",
    "In the code below, `dataset_orig` loads the COMPAS dataset. We then set numerical values for race for privileged and unprivileges groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "import wget \n",
    "import pandas as pd\n",
    "link_to_data = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n",
    "\n",
    "# make sure no duplicates\n",
    "!rm  compas-scores-two-years.csv\n",
    "#!rm /opt/conda/envs/Python36/lib/python3.6/site-packages/aif360/data/raw/compas/compas-scores-two-years.csv\n",
    "!rm /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/aif360/data/raw/compas/compas-scores-two-years.csv\n",
    "\n",
    "data_set = wget.download(link_to_data)\n",
    "\n",
    "print(data_set)\n",
    "\n",
    "compas_data_df = pd.read_csv(data_set, sep=',')\n",
    "compas_data_df.to_csv('/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/aif360/data/raw/compas/compas-scores-two-years.csv',index=False)\n",
    "#compas_data_df.to_csv('/opt/conda/envs/Python36/lib/python3.6/site-packages/aif360/data/raw/compas/compas-scores-two-years.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig = load_preproc_data_compas()\n",
    "\n",
    "privileged_groups = [{'race': 1}]\n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we split our dataset into a testing set and a training set. A training set is used to build our machine learning model. The test data set is used to assess the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape (Number of Rows , Number of Columns)\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data Visualization\n",
    "\n",
    "Before we begin building our models, we want to understand the shape of our data. In this step, you will see a visual analysis on the Compas Dataset. This cell plots out the risk score for two racial groups in focus: \"African-American\" and \"White-Caucasian\". These charts show that scores for White-Caucasian defendants were skewed toward lower-risk categories. Scores for African American defendants were not. This leads to the belief that African-American defendants are a higher risk and may re-offend more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Visualize Compas Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses plt to stand in for Matplotlib, a general purpose \"plotting\" library to create the graphs below. The series of lines below sets graph parameters and then displays the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_data = compas_data_df.loc[compas_data_df['race'] == \"African-American\"]\n",
    "wc_data = compas_data_df.loc[compas_data_df['race'] == \"Caucasian\"]\n",
    "\n",
    "af_data.hist(column=['decile_score.1'], bins=19, figsize=(10,5), xlabelsize=12, ylabelsize=12, grid = False)\n",
    "plt.xlabel(\"Risk Score\", fontsize=15)\n",
    "plt.ylabel(\"Count\",fontsize=15)\n",
    "plt.ylim([0, 700])\n",
    "plt.xlim([0.5,10.5])\n",
    "plt.xticks(np.arange(1, 11, step=1))\n",
    "plt.title(\"African-American's Risk Score\", fontsize = 18)\n",
    "\n",
    "wc_data.hist(column=['decile_score.1'], bins=19, figsize=(10,5), xlabelsize=12, ylabelsize=12, grid = False)\n",
    "plt.xlabel(\"Risk Score\", fontsize=15)\n",
    "plt.ylabel(\"Count\",fontsize=15)\n",
    "plt.ylim([0, 700])\n",
    "plt.xlim([0.5,10.5])\n",
    "plt.xticks(np.arange(1, 11, step=1))\n",
    "plt.title(\"Caucasian Defendant's Risk Score\", fontsize = 18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checking Original Dataset for Bias\n",
    "\n",
    "Now that we've identified the protected attribute and defined privileged and unprivileged values, we can use AIF360 to detect bias in the dataset. One simple test is to compare the percentage of favorable results for the privileged and unprivileged groups, subtracting the former percentage from the latter. A negative value indicates less favorable outcomes for the unprivileged groups. Within AIF360, this is implemented in the method called mean_difference on the BinaryLabelDatasetMetric class. The code below performs mean difference calculation and displays the output on both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative output indicates less favorable outcomes for the unprivileged group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Check for bias in dataset after scaling the data\n",
    "\n",
    "Scaling is a common data preparation technique to ensure that large numerical values do not overwhelm a smaller ones. In the code below, we test scaling our inputs to see if the differences between privileged and unprivileged group outcomes changes. Though we do not find a change in outcomes, this is a good practice to consider when pre-processing data for certain types of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
    "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\n",
    "metric_scaled_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\n",
    "display(Markdown(\"Note that scaling the dataset did not have an effect on the result.\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\n",
    "metric_scaled_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train on Original Dataset\n",
    "\n",
    "Here we train a Logistic Regression model on the original training data and obtain the accuracy using TensorFlow.\n",
    "\n",
    "TensorFlow is an extremely popular machine learning library. See: https://www.tensorflow.org/\n",
    "\n",
    "Obtaining the original accuracy here is necessary because we need to compare it to the accuracy obtained after bias mitigation - to make sure our model will continue to perform well.\n",
    "\n",
    "`sess = tf.Session()` sets TensorFlow to a session. The remaining code defines the parameters for our model and then trains it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess)\n",
    "plain_model.fit(dataset_orig_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Classification Metric References **\n",
    "\n",
    "Classification metric refers to a quantification of unwanted bias in models. \n",
    "\n",
    "Moving forward with the lab, you will compute and compare several classification metrics as below. Please refer to this cell when comparing metric values. Each metric value should fall within a certain range for their corresponding metric. \n",
    "\n",
    "- #### Disparate Impact\n",
    "Computed as the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group. The ideal value of this metric is 1.0 A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group. Fairness for this metric is between 0.8 and 1.2\n",
    "\n",
    "- #### Equal Opportunity Difference\n",
    "This metric is computed as the difference of true positive rates between the unprivileged and the privileged groups. The true positive rate is the ratio of true positives to the total number of actual positives for a given group. The ideal value is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group. Fairness for this metric is between -0.1 and 0.1\n",
    "\n",
    "- #### Average Odds Difference\n",
    "Computed as average difference of false positive rate (false positives / negatives) and true positive rate (true positives / positives) between unprivileged and privileged groups. The ideal value of this metric is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group. Fairness for this metric is between -0.1 and 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Classification Metrics of Plain Model - not Debiased\n",
    "\n",
    "The code below calculates and displays the results of the plain, non-debiased model for the classification metrics above (disparate impact, equal opportunity difference, and average odds difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to train and test data\n",
    "dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
    "\n",
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "metric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(dataset_nodebiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(Fore.RESET + \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(dataset_nodebiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(Fore.RESET + \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "classified_metric_nodebiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_nodebiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "\n",
    "print(Fore.RESET + \"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "if 0.8 < classified_metric_nodebiasing_test.disparate_impact() < 1.2:\n",
    "    print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "else:\n",
    "    print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact() , Fore.RED + \"      *** Bias Detected\")\n",
    "\n",
    "if  -0.1 < classified_metric_nodebiasing_test.equal_opportunity_difference() < 0.1:\n",
    "    print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "else:\n",
    "    print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference(), Fore.RED + \"      *** Bias Detected\")\n",
    "\n",
    "if  -0.1 < classified_metric_nodebiasing_test.average_odds_difference() < 0.1:\n",
    "    print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "else:\n",
    "    print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference(), Fore.RED + \"      *** Bias Detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the results. The items in red are outside the bounds of a fair model signalling that the original model is biased and that we will need to apply bias mitigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Adversarial Debiasing\n",
    "\n",
    "Adversarial debiasing is an in-processing technique that trains a classifier to maximize prediction accuracy and simultaneously reduce an adversary's ability to determine the protected attribute from the predictions [1]. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit.\n",
    "\n",
    "In this cell, you will create a Debiased Model using Adversarial Debiasing and train it on the dataset.\n",
    "\n",
    "The code below creates a new TensorFlow session and builds a machine learning model on the same data as ealier, this time with debiasing (note the `debias=True` line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Learn parameters with debias set to True\n",
    "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)\n",
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Comparing Results\n",
    "\n",
    "This step calculates and prints out the result of the bias mitigation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)\n",
    "\n",
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "print(Fore.RESET + \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "print(Fore.RESET + \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(Fore.RESET + \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(Fore.RESET + \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(Fore.RESET + \"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(Fore.RESET + \"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "\n",
    "if 0.8 < classified_metric_nodebiasing_test.disparate_impact() < 1.2:\n",
    "    print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "else:\n",
    "    print(Fore.RED + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "\n",
    "if  -0.1 < classified_metric_nodebiasing_test.equal_opportunity_difference() < 0.1:\n",
    "    print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "else:\n",
    "    print(Fore.RED + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "\n",
    "if  -0.1 < classified_metric_nodebiasing_test.average_odds_difference() < 0.1:\n",
    "    print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "else:\n",
    "    print(Fore.RED + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "\n",
    "# print(Fore.RESET + \"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_debiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "print(Fore.RESET + \"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact(), Fore.GREEN + \"    *** Fairness Achieved ***\")\n",
    "print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference(), Fore.GREEN + \"    *** Fairness Achieved ***\")\n",
    "print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference(), Fore.GREEN + \"    *** Fairness Achieved ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines printed in red show bias in the original model and the lines with green writing \"Fairness Achieved\" mean that the bias mitigation algorithm was successfuly applied fairness to the model and the classification metric results have moved closer to the acceptable range. In a best case scenario they will fall in the desired range. This means fairness is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Visualizing Results\n",
    "\n",
    "Let's visualize the results as well. The code below takes our calculations and displays charts comparing the original results to the results with bias mitigation applied.\n",
    "\n",
    "We first calculate the original and mitigated results using each fairness metric (disparate impact, opportunity difference, and average odds difference):\n",
    "\n",
    "Example for disparate impact:\n",
    "\n",
    "`orig_DI = classified_metric_nodebiasing_test.disparate_impact()`\n",
    "\n",
    "`mitigated_DI = classified_metric_debiasing_test.disparate_impact()`\n",
    "\n",
    "We then use Matplotlib, assigned to `plt` to chart our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_DI = classified_metric_nodebiasing_test.disparate_impact()\n",
    "mitigated_DI = classified_metric_debiasing_test.disparate_impact()\n",
    "\n",
    "orig_EOD = classified_metric_nodebiasing_test.equal_opportunity_difference()\n",
    "mitigated_EOD = classified_metric_debiasing_test.equal_opportunity_difference()\n",
    "\n",
    "orig_AOD = classified_metric_nodebiasing_test.average_odds_difference()\n",
    "mitigated_AOD = classified_metric_debiasing_test.average_odds_difference()\n",
    "\n",
    "N = 2\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.5       # the width of the bars: can also be len(x) sequence\n",
    "xtype = ('Original', 'Mitigated')\n",
    "ind = np.arange(len(xtype))\n",
    "\n",
    "# Disparate Impact\n",
    "\n",
    "values_DI = [orig_DI, mitigated_DI]\n",
    "# print(\"values_DI =  \", values_DI, end='\\t')\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "p1_DI = plt.bar(ind, values_DI, width = width, color=['darkgray', 'dodgerblue'], edgecolor='black') #, yerr=menStd)\n",
    "plt.xticks(ind, xtype)\n",
    "plt.grid(b=None, which='major', axis='y', linestyle=':', linewidth=1)\n",
    "plt.ylabel('Value')\n",
    "plt.title('Disparate Impact', fontsize=15)\n",
    "plt.ylim([-0.4, 1.2])\n",
    "plt.yticks(np.arange(-0.4, 1.3, 0.1)) \n",
    "plt.axhspan(ymin = 0.8, ymax = 1.2, color='green', alpha=0.1)\n",
    "plt.axhspan(ymin = 0, ymax = 0, edgecolor='black', alpha=1)\n",
    "# plt.legend(xtype)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "# Equal Opprortunity Difference\n",
    "\n",
    "values_EOD = [orig_EOD, mitigated_EOD]\n",
    "# print(\"values_EOD = \", values_EOD, end='\\t')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "p1_EOD = plt.bar(ind, values_EOD, width = width, color=['darkgray', 'dodgerblue'], edgecolor='black')\n",
    "plt.xticks(ind, xtype)\n",
    "plt.grid(b=None, which='major', axis='y', linestyle=':', linewidth=1)\n",
    "plt.ylabel('Value')\n",
    "plt.title('Equal Opportunity Difference', fontsize=15)\n",
    "plt.ylim([-0.4, 1.2])\n",
    "plt.yticks(np.arange(-0.4, 1.3, 0.1))\n",
    "plt.axhspan(ymin = -0.1, ymax = 0.1, color='green', alpha=0.1)\n",
    "plt.axhspan(ymin = 0, ymax = 0, edgecolor='black', alpha=1)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "# Average odds difference\n",
    "\n",
    "values_AOD = [orig_AOD, mitigated_AOD]\n",
    "# print(\"values_AOD = \", values_AOD)\n",
    "\n",
    "ax = plt.subplot(1,3,3)\n",
    "p1_AOD = plt.bar(ind, values_AOD, width = width, color=['darkgray', 'dodgerblue'], edgecolor='black') #, yerr=menStd)\n",
    "plt.xticks(ind, xtype)\n",
    "plt.grid(b=None, which='major', axis='y', linestyle=':', linewidth=1)\n",
    "plt.ylabel('Value')\n",
    "plt.title('Average Odds Difference', fontsize=15)\n",
    "plt.ylim(-0.4, 1.2)\n",
    "plt.yticks(np.arange(-0.4, 1.3, 0.1))\n",
    "plt.axhspan(ymin = -0.1, ymax = 0.1, color='Green', alpha=0.1)\n",
    "plt.axhspan(ymin = 0, ymax = 0, edgecolor='black', alpha=1)\n",
    "plt.subplots_adjust(top=1, bottom=0.08, left=0.10, right=2.5, hspace=0.5, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell demonstrates the result of the bias mitigation on graphs using the difference fairness measures. \n",
    "\n",
    "The blue bar is the mitigated result\n",
    "\n",
    "The gray bar is the original (base case).\n",
    "\n",
    "Notice that the blue bar (with bias mitigation) is closer to the desired range and in the base case scenario, it falls in the desired range. This means that fairness is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating UnwantedBiases with Adversarial Learning,\" \n",
    "AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
